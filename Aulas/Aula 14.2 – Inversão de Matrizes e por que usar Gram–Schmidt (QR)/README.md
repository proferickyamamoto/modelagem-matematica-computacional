# Aula 14.2 â€“ InversÃ£o de Matrizes e por que usar Gramâ€“Schmidt (QR)

---

## ğŸ¯ Objetivos

* Entender **quando** e **como** uma matriz Ã© invertÃ­vel e as consequÃªncias de sua inversa.
* Comparar mÃ©todos: **Gaussâ€“Jordan**, **adjunta** e **fatoraÃ§Ãµes** (LU/QR).
* Compreender **condicionamento** e por que **evitar** calcular $A^{-1}$ explicitamente.
* Justificar o uso de **Gramâ€“Schmidt/QR** para resolver sistemas e **mÃ­nimos quadrados** com mais estabilidade.
* Implementar as tÃ©cnicas em **Octave/Matlab**, **Python (NumPy)** e **R**.

---

## ğŸ“˜ Fundamentos

### 1) InversÃ£o: definiÃ§Ã£o e existÃªncia

Para $A\in\mathbb{R}^{n\times n}$, dizemos que $A$ Ã© **invertÃ­vel** se existe $A^{-1}$ tal que
$A\,A^{-1}=A^{-1}A=I.$
SÃ£o equivalentes:

* $\det(A)\neq 0$
* $posto\{(A)}=n$
* O sistema $A\,x=b$ tem **Ãºnica** soluÃ§Ã£o para todo $b\in\mathbb{R}^n$
* As colunas de $A$ sÃ£o LI (base de $\mathbb{R}^n$)

**Propriedades Ãºteis:**

* $(AB)^{-1}=B^{-1}A^{-1}$
* $(A^T)^{-1}=(A^{-1})^T$
* $\det(A^{-1})=1/\det(A)$

### 2) Como inverter (teoria x prÃ¡tica)

* **Gaussâ€“Jordan**: resolve $[A\,|\,I] \to [I\,|\,A^{-1}]$. Custo $\mathcal{O}(n^3)$.
* **Adjunta**: $A^{-1}=\tfrac{adj\{(A)}}{\det(A)}$ (bom para prova/2Ã—2/3Ã—3; **impraticÃ¡vel** para $n$ grande).
* **FatoraÃ§Ãµes**: use **LU**/**QR** para **resolver** $A\,x=b$ sem formar $A^{-1}$.

### 3) Por que **nÃ£o** usar $\mathbf{inv(A)}$ diretamente?

* **Custo** e **erro numÃ©rico**: formar $x=A^{-1}b$ Ã© mais caro e menos estÃ¡vel que resolver $A\,x=b$ por fatoraÃ§Ã£o.
* **Condicionamento**: erro relativo amplificado por $\kappa(A)=\|A\|\,\|A^{-1}\|$. Se $\kappa(A)$ Ã© alto, pequenas perturbaÃ§Ãµes $\Rightarrow$ grandes erros em $x$.
* **Regra de ouro** (Strang; Trefethen & Bau): *nunca* use `inv(A)*b`; prefira $A\\b$ (Octave/Matlab), `solve(A,b)` (NumPy/R).

### 4) Onde entra o **Gramâ€“Schmidt/QR**?

* Para $A\in\mathbb{R}^{m\times n}$ (quadrada ou â€œaltaâ€), fatoramos $A=QR$ com $Q$ **ortonormal** e $R$ triangular superior.
* Resolver $A\,x=b$: $QRx=b\Rightarrow Rx=Q^T b$ (duas etapas estÃ¡veis).
* **MÃ­nimos quadrados** $(m>n)$: evita as **equaÃ§Ãµes normais** $(A^TA)x=A^Tb$, que **pioram** o condicionamento: $\kappa(A^TA)=\kappa(A)^2$.
* **Gramâ€“Schmidt** (melhor, **modificado**) constrÃ³i $Q$ e $R$: base ortonormal do espaÃ§o das colunas de $A$ $\Rightarrow$ projeÃ§Ãµes e soluÃ§Ãµes estÃ¡veis.

> **Resumo prÃ¡tico**: Calcular inversa Ã© muitas vezes **didÃ¡tico**; em aplicaÃ§Ãµes, resolva com **LU/QR**. Use **QR/Gramâ€“Schmidt** em mÃ­nimos quadrados e quando a estabilidade importa.

---

## ğŸ§® Exemplo guiado (3Ã—3): Gaussâ€“Jordan x QR

Considere
$A=\begin{bmatrix} 2 & 1 & 0\\ 1 & 3 & 1\\ 0 & 1 & 2\end{bmatrix},\quad b=\begin{bmatrix}1\\2\\3\end{bmatrix}.$

1. **Gaussâ€“Jordan**: obtenha $A^{-1}$ e calcule $x=A^{-1}b$.
2. **QR**: fatorar $A=QR$; resolva $Rx=Q^Tb$. Compare tempo/estabilidade (condiÃ§Ã£o de $A$).

---

## ğŸ’» CÃ³digo

### Octave/Matlab â€“ Gaussâ€“Jordan, Solve e QR

```octave
% Inversa por Gaussâ€“Jordan (didÃ¡tico)
function Ainv = gauss_jordan_inv(A)
  [n, m] = size(A); assert(n==m, 'A deve ser quadrada');
  AI = [A eye(n)];
  for j = 1:n
    % pivÃ´ parcial simples
    [~,p] = max(abs(AI(j:n,j))); p = p + j - 1;
    AI([j p],:) = AI([p j],:);
    % normaliza pivÃ´
    AI(j,:) = AI(j,:) / AI(j,j);
    % zera demais linhas
    for i = 1:n
      if i ~= j
        AI(i,:) = AI(i,:) - AI(i,j)*AI(j,:);
      end
    end
  end
  Ainv = AI(:, n+1:end);
end

A = [2 1 0; 1 3 1; 0 1 2]; b = [1;2;3];
Ainv = gauss_jordan_inv(A);
x_inv = Ainv*b;

% Forma correta/estÃ¡vel
x_solve = A\b;           % usa LU/QR internamente
[Q,R] = qr(A); x_qr = R\(Q'*b);

fprintf('||x_inv - x_solve|| = %g\n', norm(x_inv - x_solve));
condA = cond(A); fprintf('cond(A) = %g\n', condA);
```

### Python (NumPy)

```python
import numpy as np

A = np.array([[2.,1.,0.],
              [1.,3.,1.],
              [0.,1.,2.]])
b = np.array([1.,2.,3.])

# Evitar: x = np.linalg.inv(A) @ b
x_solve = np.linalg.solve(A, b)
Q, R = np.linalg.qr(A, mode='reduced')
x_qr = np.linalg.solve(R, Q.T @ b)

print('x_solve =', x_solve)
print('x_qr    =', x_qr)
print('cond(A) =', np.linalg.cond(A))
```

### R

```r
A <- matrix(c(2,1,0, 1,3,1, 0,1,2), nrow=3, byrow=TRUE)
b <- c(1,2,3)

# Evitar: solve(A) %*% b
x_solve <- solve(A, b)     # resoluÃ§Ã£o estÃ¡vel
qrA <- qr(A)
x_qr <- backsolve(qr.R(qrA), t(qr.Q(qrA)) %*% b)

print(x_solve)
print(x_qr)
print(kappa(A))
```

### MÃ­nimos quadrados: normal vs QR (perigo do condicionamento)

**Python (NumPy)**

```python
import numpy as np
np.random.seed(0)
# colunas quase colineares => mal condicionado
x = np.linspace(0,1,20)
X = np.column_stack([np.ones_like(x), x, x + 1e-4*np.random.randn(len(x))])
y = 2 + 3*x + 0.1*np.random.randn(len(x))

# EquaÃ§Ãµes normais (evitar)
beta_ne = np.linalg.solve(X.T@X, X.T@y)
# QR (preferÃ­vel)
Q, R = np.linalg.qr(X, mode='reduced')
beta_qr = np.linalg.solve(R, Q.T @ y)

print('cond(X)   =', np.linalg.cond(X))
print('cond(X^T X)=', np.linalg.cond(X.T@X))
print('beta_ne   =', beta_ne)
print('beta_qr   =', beta_qr)
```

---

## ğŸ§  Por que o **Gramâ€“Schmidt** (QR) Ã© a escolha certa?

* **Ortonormalidade** $\Rightarrow$ projeÃ§Ãµes simples: $\hat y=QQ^Ty$.
* **Estabilidade**: evita $A^TA$ e seu condicionamento quadrÃ¡tico; sensÃ­vel a ruÃ­do? **Menos** com QR.
* **Generalidade**: resolve $A\,x=b$ (quadrada) e $\min_x\|Ax-b\|$ (retangular) com o **mesmo** esquema $Rx=Q^Tb$.
* **InterpretaÃ§Ã£o geomÃ©trica**: Gramâ€“Schmidt constrÃ³i uma **base ortonormal** do espaÃ§o das colunas â€“ enxergamos projeÃ§Ãµes e componentes.

> ObservaÃ§Ã£o: Em produÃ§Ã£o, muitas bibliotecas usam **Householder** (ainda mais estÃ¡vel). O **Gramâ€“Schmidt modificado** Ã© uma boa ponte didÃ¡tica.

---

## ğŸ« ExercÃ­cios em Sala

1. Inverta por **Gaussâ€“Jordan**: $\small A=\begin{bmatrix}1&2&1\\0&1&1\\2&1&0\end{bmatrix}$ e verifique $AA^{-1}=I$.
2. Resolva $A\,x=b$ com (a) `inv(A)*b` e (b) $A\\b$. Compare $\|x_a-x_b\|$ e relate com $\kappa(A)$.
3. Para dados $(x_i,y_i)$, ajuste $y\approx\beta_0+\beta_1x$ com (a) normais e (b) QR. Compare coeficientes e resÃ­duos.

---

## ğŸ  Tarefa (10 itens)

1. Demonstre: $(AB)^{-1}=B^{-1}A^{-1}$.
2. Prove: $\det(A^{-1})=1/\det(A)$.
3. Mostre que $\kappa(A)\ge 1$ e interprete.
4. Implemente Gaussâ€“Jordan e teste em matrizes 2Ã—2 e 3Ã—3.
5. Gere uma matriz quase singular e meÃ§a erros de `inv(A)*b` vs `solve`.
6. Compare mÃ­nimos quadrados por **normais** vs **QR** em um dataset com colunas quase colineares.
7. A partir de $A=QR$, mostre que $Q$ tem colunas ortonormais.
8. Explique por que **equaÃ§Ãµes normais** quadruplicam o condicionamento ($\kappa(A^TA)=\kappa(A)^2$).
9. Use **Gramâ€“Schmidt modificado** para construir $Q,R$ e valide com `qr`.
10. Leitura comentada: Strang (cap. QR) e Trefethen & Bau (caps. 9â€“11).

---

## ğŸ“š ReferÃªncias

* **Strang, G.** *Introduction to Linear Algebra*, Wellesleyâ€“Cambridge Press.
* **Trefethen, L. N.; Bau, D.** *Numerical Linear Algebra*, SIAM.
* **Golub, G.; Van Loan, C.** *Matrix Computations*, Johns Hopkins.
* **Boyd, S.; Vandenberghe, L.** *Introduction to Applied Linear Algebra*, Cambridge.

---

**âœ… Moral da aula:** *Inversa Ã© conceito central, mas em prÃ¡tica resolvemos com **fatoraÃ§Ãµess** (LU/QR). Para estabilidade e mÃ­nimos quadrados, **QR/Gramâ€“Schmidt** ganha de longe das equaÃ§Ãµes normais.*
